{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda8472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import os.path\n",
    "import json\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a08d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipe():\n",
    "    \n",
    "    def __init__(self, name, url, ingredients):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.ingredients = ingredients\n",
    "        \n",
    "    def __str__(self):\n",
    "        return(self.name)\n",
    "    \n",
    "    \n",
    "def get_recipes(driver, link):\n",
    "        next_page_exists = True\n",
    "        \n",
    "        print(f'Getting recipes from {link}')\n",
    "        logging.info(f'Getting recipes from {link}')\n",
    "        driver.get(link)\n",
    "    \n",
    "        # set maximum time to load the web page in seconds\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        while next_page_exists:\n",
    "            # collect data that are within the id 'recipes-page'\n",
    "            recipes_page = driver.find_element(By.ID, 'recipes-page')\n",
    "            # Get all the recipe links\n",
    "            recipes = recipes_page.find_elements(By.CLASS_NAME, 'c-recipe-grid__item')\n",
    "\n",
    "            for recipe in recipes:\n",
    "                name = recipe.find_element(By.TAG_NAME, 'span').text\n",
    "                url = recipe.get_attribute('href')\n",
    "                page = requests.get(url)\n",
    "                all_recipes_dict[name] = {'url': url, 'ingredients': get_ingredients(page)}\n",
    "                # class_=None because we don't want to retrieve the measurements\n",
    "            try: \n",
    "                # Get \"load more\" button to get to the next page\n",
    "                button = recipes_page.find_element(By.CLASS_NAME, 'c-button')\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                # We reached the last page, the \"load more\" button does not exist\n",
    "                logging.info(f'Reached the last page of {link}')\n",
    "                next_page_exists = False\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                driver.execute_script(\"arguments[0].click();\", button)\n",
    "                time.sleep(5) \n",
    "\n",
    "def get_ingredients(page):\n",
    "    ingredients = [str(ingredient.contents[0]) for ingredient in BeautifulSoup(page.text, 'html.parser').find_all('td', class_=None)]\n",
    "    return ingredients                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d993b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website to scrape\n",
    "links = [\n",
    "    'https://ottolenghi.co.uk//pages/mains-recipes',\n",
    "    'https://ottolenghi.co.uk/pages/sides-recipes',\n",
    "    'https://ottolenghi.co.uk/pages/soup-recipes',\n",
    "    'https://ottolenghi.co.uk/pages/salad-recipes',\n",
    "    ]\n",
    "\n",
    "jsn_file = f'../data/interim/ottolenghi_recipes.json'\n",
    "\n",
    "# If the json file exists, we do not get the data from the website again\n",
    "if os.path.isfile(jsn_file):\n",
    "    logging.info('Data has already been scrapped')\n",
    "    logging.info('Downloading json file')\n",
    "    with open(jsn_file, 'rb') as f:\n",
    "        all_recipes_dict = json.load(f)\n",
    "# If not, get the data from the website\n",
    "else: \n",
    "    all_recipes_dict = {}\n",
    "\n",
    "    # Installing webdriver\n",
    "    logging.info('Installing Chrome webdriver')  \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    \n",
    "    # load the web page\n",
    "    for link in links: \n",
    "        get_recipes(driver, link)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    with open(jsn_file, 'w') as f:\n",
    "        json.dump(all_recipes_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80544e9-7fd5-41cb-b337-cd4773b24242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(all_recipes_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9cf8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
